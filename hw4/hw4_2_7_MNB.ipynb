{"cells":[{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","from sklearn.metrics import confusion_matrix\n","from sklearn.feature_extraction.text import CountVectorizer"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[H\u001b[2J"]}],"source":["import os\n","np.set_printoptions(suppress=True)\n","os.system('cls' if os.name == 'nt' else 'clear') # Clean the console\n","input_loc = os.getcwd() "]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["X_train = list()\n","X_test = list()\n","y_train = list()\n","y_test = list()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["languages = ['e', 'j', 's']\n","for class_index in range(3):\n","    for file_index in range(10):\n","        file_name = input_loc +'/languageID/'+ languages[class_index] + str(file_index)+'.txt'\n","        X_train.append(open(file_name,'r').read()) # Read all text while ignoring \"new line\"\n","        y_train.append(class_index)\n","    for file_index in range(10,20):\n","        file_name = input_loc +'/languageID/'+ languages[class_index] + str(file_index)+'.txt'\n","        X_test.append(open(file_name,'r').read()) # Read all text while ignoring \"new line\"\n","        y_test.append(class_index)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["y_train = np.asarray(y_train)\n","y_test = np.asarray(y_test)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["vectorizer = CountVectorizer(analyzer='char')"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["X_train = vectorizer.fit_transform(X_train).toarray()\n","X_test = vectorizer.fit_transform(X_test).toarray()\n","# print(X_train[:5,0:10])"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Swap \"space\" to the last column of the feature set\n","X_train = np.concatenate((X_train[:,1:],X_train[:,0].reshape((len(X_train[:,-1]),1))), axis=1)\n","X_test = np.concatenate((X_test[:,1:],X_test[:,0].reshape((len(X_test[:,-1]),1))), axis=1)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["from naive_bayes import MultinomialNaiveBayes"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["mnb = MultinomialNaiveBayes(X_train, y_train)\n","mnb.train(X_train, y_train)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2]\n","[[10  0  0]\n"," [ 0 10  0]\n"," [ 0  0 10]]\n"]}],"source":["y_pred = mnb.predict(X_test)\n","print(y_pred)\n","print(confusion_matrix(y_test, y_pred))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":2}
